{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLG: Lab 4\n",
    "\n",
    "Throughout the laboratories, questions that you should try to answer are highlighted as follow :\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: This is a question</p>\n",
    "\n",
    "The file to open is [Lab4_Entropy_DecisionTree.ipynb](./Lab4_Entropy_DecisionTree.ipynb)\n",
    "\n",
    "# Report\n",
    "\n",
    "We ask you a basic report answering the following constraints:\n",
    "- You should answer all purple questions. Note that if you include your development, we can award you points for it. If you just put your answer (and if it is wrong), we will not be able to give you any points.\n",
    "- You should introduce and conclude each question with whatever you understand about it\n",
    "- Report is limited to 5 pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H_b(X)= -\\mathbb E [\\log_b {P(X=x_i)}] = \\sum_{i=1}^nP_i\\log_b \\left(\\frac{1}{P_i}\\right)=-\\sum_{i=1}^nP_i\\log_b P_i\\,\\!$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, let us look at the Wikipedia example:\n",
    "\n",
    "https://fr.wikipedia.org/wiki/Entropie_de_Shannon#Tirage_al.C3.A9atoire_dans_une_urne\n",
    "\n",
    "Let us consider an bowl containing a red, a blue, a yellow and a green ball. We choose a random ball. No color being preferred (each having one chance over four of being chosen), the entropy is at its maximum, here equal to:\n",
    "\n",
    "$$H(x) = -\\sum_{i=1}^nP_i\\log_b P_i = -\\sum_{i=1}^41/4\\log_2(1/4) = - 4*1/4\\log_2(1/4) = -\\log_2(1/4) = \\log_2(4) = 2$$\n",
    "\n",
    "If we agree that the colors are coded respectively: 00, 01, 10, 11, the information contained in the choice is indeed corresponding to 2 bits.\n",
    "\n",
    "In the case where a color is more represented than others, then the entropy is slightly reduced. Let us suppose for example that the bowl contains 4 red balls, 2 blue balls, 1 yellow and 1 green. The entropy is therefore equal to\n",
    "\n",
    "$$H(x)=7/4$$\n",
    "\n",
    "Indeed,\n",
    "\n",
    "$$\\begin{align}H(x)\n",
    "&= - \\frac{4}{8}\\log_2\\left(\\frac{4}{8}\\right) - \\frac{2}{8} \\log_2\\left(\\frac{2}{8}\\right) - \\frac{1}{8} \\log_2\\left(\\frac{1}{8}\\right) - \\frac{1}{8} \\log_2\\left(\\frac{1}{8}\\right) \\\\\n",
    "&= - \\frac{\\log_2(1/2)}{2} - \\frac{\\log_2(1/4)}{4} - \\frac{\\log_2(1/8)}{8} - \\frac{\\log_2(1/8)}{8} \\\\\n",
    "&= \\frac{\\log_2(2)}{2} + \\frac{\\log_2(4)}{4} + \\frac{\\log_2(8)}{8} + \\frac{\\log_2(8)}{8} \\\\\n",
    "&= 1/2 + 2/4 + 3/8 + 3/8 = 7/4\n",
    "\\end{align}.$$\n",
    "\n",
    "Easier than expected, no ? Let us see how entropy is used for building decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example: Use entropy to build a Decision Tree Classifier\n",
    "\n",
    "A decision tree is a recursive partitioning methodology that uses entropy to compute the best split at each node. A split is an action that separates the data in two parts. Intuitively, we could say that the tree will repeat this procedure:\n",
    "1. Compute the entropy of the observations at the current node\n",
    "2. For all possible splits:\n",
    "    1. Compute the entropy at the left-child node\n",
    "    2. Compute the entropy at the right-child node\n",
    "    3. Sum them. This gives the split's entropy\n",
    "3. Choose the split with the minimum entropy\n",
    "\n",
    "To see this in action, let us build a basic decision tree step by step:\n",
    "\n",
    "First, let's imagine that we have some points belonging to 3 classes in a 2d space. For example, let's say they are defined like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tVar1\tVar2\tClass\n",
      "Point 0:\t0\t1\t0\n",
      "Point 1:\t0\t0\t0\n",
      "Point 2:\t1\t1\t1\n",
      "Point 3:\t1\t0\t2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEDCAYAAADdpATdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDZ0fDwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f561f753750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "colors = {0: 'r', 1: 'g', 2: 'b'}\n",
    "\n",
    "points = np.array(([0,1,0],\n",
    "                   [0,0,0],\n",
    "                   [1,1,1],\n",
    "                   [1,0,2]))\n",
    "print '\\t\\tVar1\\tVar2\\tClass'\n",
    "for i, point in enumerate(points):\n",
    "    print 'Point '+str(i)+':\\t'+str(point[0])+'\\t'+str(point[1])+'\\t'+str(point[2])\n",
    "\n",
    "for class_no in np.unique(points[:,2]):\n",
    "    filtered_points = points[points[:, 2]==class_no, :]\n",
    "    pl.scatter(filtered_points[:, 0], filtered_points[:, 1], label='Class '+str(class_no), c=colors[class_no])\n",
    "legend = pl.legend(loc=5, frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_edgecolor('white')\n",
    "frame.set_linewidth(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the entropy, which is now:\n",
    "\n",
    "$$-(1/2*log_2(1/2)+1/4*log_2(1/4)+1/4*log_2(1/4)))=1.5$$\n",
    "\n",
    "Now, we have two available variables to split on.\n",
    "Let's try both splits:\n",
    "\n",
    "**Split on Var1:** $$Var1==0.5$$\n",
    "If we cut at 0.5 (meaning all observations with Var1 <= 0.5 will go to left-child node, while all observations with Var1 > 0.5 will go to right-child node), we find ourselves with Points 0 and 1 at the left node, and Points 2 and 3 in the right node.\n",
    "\n",
    "We can therefore compute the entropy at the left node:\n",
    "$$-(1*log_2(1))=0$$\n",
    "\n",
    "Obviously, no more entropy exists at this point, since all observations are of class 0.\n",
    "\n",
    "And at the right node:\n",
    "$$-(1/2*log_2(1/2)+1/2*log_2(1/2))=-(log_2(1/2))=1$$\n",
    "\n",
    "Therefore, the entropy gain by splitting on Var1 was:\n",
    "$$1.5 - ((0 + 1)/2) = 1.5 - 0.5 = 1$$\n",
    "\n",
    "**Split on Var2:** $$Var2==0.5$$\n",
    "If we cut at 0.5, we find ourselves with Points 1 and 3 at the left node, and Points 0 and 2 in the right node.\n",
    "\n",
    "We can therefore compute the entropy at the left node:\n",
    "$$-(1/2*log_2(1/2)+1/2*log_2(1/2))=1$$\n",
    "\n",
    "And at the right node:\n",
    "$$-(1/2*log_2(1/2)+1/2*log_2(1/2))=1$$\n",
    "\n",
    "Therefore, the entropy gain by splitting on Var2 was:\n",
    "$$1.5 - ((1 + 1)/2) = 1.5 - 1 = 0.5$$\n",
    "\n",
    "So the best split (i.e. the one with the higher entropy gain) is the split on Var1!\n",
    "\n",
    "A decision tree stops when each leaf (terminal node) contains only 1 class. In this case, the left node contains only samples from class 0, so no further split is necessary. On the contrary, the right node contains a sample from class 1 and a sample from class 2.\n",
    "\n",
    "So we need one more split on the right node... Again, we can split on Var1 and Var2. But the split on Var1 is trivial, since we can't find a value of Var1 that separates the samples. So the entropy gain will be 0 if we split on Var1.\n",
    "\n",
    "If we split on Var2 (==0.5), we have Point 3 at the left node, and Point 2 in the right node. Therefore, the entropy at the left node is 0 ($$-1*log_2(1)$$) and the entropy at the right node is also 0 (same calculus). The entropy gain is 1 (1 - ((0+0)/2)).\n",
    "\n",
    "Therefore the split on Var2 is the best one in the right node.\n",
    "\n",
    "We find ourselves with a tree looking like this (going to the left means satisfying the condition):\n",
    "\n",
    "                 Entropy: 1.5\n",
    "              Points: [0, 1, 2, 3]\n",
    "             Classes: [0, 0, 1, 2]\n",
    "                     |\n",
    "                     |\n",
    "                     |\n",
    "                 Var1<=0.5\n",
    "                /         \\\n",
    "               /           \\\n",
    "              /             \\\n",
    "         Entropy: 0        Entropy: 1\n",
    "       Points: [0, 1]    Points: [2, 3]\n",
    "       Classes: [0, 0]   Classes: [1, 2]\n",
    "                                |\n",
    "                                |\n",
    "                                |\n",
    "                            Var2<=0.5\n",
    "                           /         \\\n",
    "                          /           \\\n",
    "                         /             \\\n",
    "                    Entropy: 0      Entropy: 0\n",
    "                    Points: [3]     Points: [2]\n",
    "                    Classes: [2]    Classes: [1]\n",
    "\n",
    "\n",
    "This is the reason why we consider decision trees a \"white-box\" model. It is easy to understand and to visualize.\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "We can now try with an automated Decision Tree Classifier building...\n",
    "\n",
    "Let's do it in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "tree = tree.fit(points[:,:-1], points[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous cell, we asked the tree to fit the Class variable of our points (points[:,-1]) using both Var1 and Var2 values as splitting possibilities (points[:,:-1]).\n",
    "\n",
    "The tree is now complete, and we can have a look at it by exporting it to the \".dot\" format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export_graphviz(tree, out_file='test.dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go on http://www.webgraphviz.com/ and copy-paste the content of the \"test.dot\" file in the box.\n",
    "\n",
    "You can look at the tree and compare it to the one we built by hand.\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: What does the \"value\" parameter indicates in the resulting tree of WebGraphViz ?</p>\n",
    "\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: What about the \"X[0]\" and \"X[1]\" parameters ?</p>\n",
    "\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: How much leaves does our tree have ? Which one are they ?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Optimize a game of Guess Who"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Guess Who](./guess-who.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to determine the best questions to ask in order to win guess who in the least number of moves!\n",
    "\n",
    "Note that in the general case, building a tree is what you are naturally doing when playing guess who!\n",
    "\n",
    "In this case, we are going to make the tree building process step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beard</th>\n",
       "      <th>Hair</th>\n",
       "      <th>Real</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Brown</td>\n",
       "      <td>No</td>\n",
       "      <td>Male</td>\n",
       "      <td>Dumbledore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Male</td>\n",
       "      <td>Woody Allen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Female</td>\n",
       "      <td>Beyoncé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Blond</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ryan Gosling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>Blond</td>\n",
       "      <td>No</td>\n",
       "      <td>Female</td>\n",
       "      <td>Daenerys Targaryen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No</td>\n",
       "      <td>Brown</td>\n",
       "      <td>No</td>\n",
       "      <td>Female</td>\n",
       "      <td>Lara Croft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Male</td>\n",
       "      <td>Karl Marx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>No</td>\n",
       "      <td>Brown</td>\n",
       "      <td>No</td>\n",
       "      <td>Male</td>\n",
       "      <td>San Goku</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Beard   Hair Real     Sex               Names\n",
       "0   Yes  Brown   No    Male          Dumbledore\n",
       "1    No  Brown  Yes    Male         Woody Allen\n",
       "2    No  Brown  Yes  Female             Beyoncé\n",
       "3   Yes  Blond  Yes    Male        Ryan Gosling\n",
       "4    No  Blond   No  Female  Daenerys Targaryen\n",
       "5    No  Brown   No  Female          Lara Croft\n",
       "6   Yes  Brown  Yes    Male           Karl Marx\n",
       "7    No  Brown   No    Male            San Goku"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "        'Names': ['Dumbledore', 'Woody Allen', 'Beyoncé', 'Ryan Gosling',\n",
    "                  'Daenerys Targaryen', 'Lara Croft', 'Karl Marx', 'San Goku'\n",
    "                 ],\n",
    "        'Real': ['No', 'Yes', 'Yes', 'Yes',\n",
    "                 'No', 'No', 'Yes', 'No'\n",
    "                ],\n",
    "        'Sex': ['Male', 'Male', 'Female', 'Male',\n",
    "                'Female', 'Female', 'Male', 'Male'\n",
    "               ],\n",
    "        'Beard': ['Yes', 'No', 'No', 'Yes',\n",
    "                  'No', 'No', 'Yes', 'No'\n",
    "                 ],\n",
    "        'Hair': ['Brown', 'Brown', 'Brown', 'Blond',\n",
    "                 'Blond', 'Brown', 'Brown', 'Brown'\n",
    "                ]\n",
    "    }, columns=['Beard', 'Hair', 'Real', 'Sex', 'Names'])\n",
    "\n",
    "df.Names = pd.Categorical(df.Names)\n",
    "df.Real = pd.Categorical(df.Real)\n",
    "df.Sex = pd.Categorical(df.Sex)\n",
    "df.Beard = pd.Categorical(df.Beard)\n",
    "df.Hair = pd.Categorical(df.Hair)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in that case, the \"classes\" are the names!\n",
    "\n",
    "Also note that even if it doesn't seem so, the variables (Hair, Beard, Sex, Real) are all 0-1 (binary) variables, like in our previous example. Therefore, you can split at 0.5 as well. (You can decide which value is 1 and which value is 0)\n",
    "\n",
    "Therefore, let's go and build the tree ourselves...\n",
    "\n",
    "***\n",
    "\n",
    "First, let's determine the entropy at the root node:\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: What is the value of the entropy at the root node ?</p>\n",
    "\n",
    "Then, let's try to split on the four variables.\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: What is the value of the entropy in the both child nodes (left and right) if you decide to split on the \"Beard\" variable ?</p>\n",
    "\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: Same question for the \"Hair\" variable ?</p>\n",
    "\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: Same question for the \"Sex\" variable ?</p>\n",
    "\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: Same question for the \"Real\" variable ?</p>\n",
    "\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: Therefore, what is the best variable to split on ? Why ?</p>\n",
    "\n",
    "Now that we have our first split, we can do the same for all subsequent splits.\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: Finish building the tree, and for each split variable, give the entropy in both child nodes as well as the resulting entropy gain. You can do it by hand or by code!</p>\n",
    "\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: Are there more than one optimal solution ? Explain why...</p>\n",
    "\n",
    "Now, let's do it the automated way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "# We select all columns, but the \"Names\" one. This creates input_df\n",
    "input_df = df.select(lambda col: col != 'Names', axis=1)\n",
    "for col in input_df.columns:\n",
    "    input_df[col] = input_df[col].cat.codes\n",
    "\n",
    "# The \"Names\" column is used in the output_df\n",
    "output_df = df.Names.cat.codes\n",
    "\n",
    "# We train our decision tree classifier with our input_df onto our output_df\n",
    "tree_model = DecisionTreeClassifier(criterion='entropy')\n",
    "tree_model = tree_model.fit(input_df.as_matrix(), output_df.as_matrix())\n",
    "\n",
    "# And again, we can export it to a dot file and examine it \n",
    "export_graphviz(tree_model, out_file='guess_who.dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the generated tree in http://www.webgraphviz.com/ !\n",
    "\n",
    "Does it look like the one you computed by hand ? If not, try to understand where you did something wrong.\n",
    "\n",
    "For more information, you can look at the documentation on scikit-learn.org\n",
    "\n",
    "For example here is an example on the well-known iris dataset: http://scikit-learn.org/stable/auto_examples/tree/plot_iris.html#example-tree-plot-iris-py\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: How many classes are there in that example ?</p>\n",
    "\n",
    "You can see that the separations are mostly straight lines (we see for example in the top-right figure that the blue class seems to be well separated by a line when \"petal width (cm) <= -0.5\"). By the way, in that example the values are standardized (the mean of each value is reduced to 0 and the standard deviation is set to 1). It is useful for many models, but not for Decision Trees.\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: What are the possible splits on a real variable (i.e. not 0-1 (binary)), such as the variables in the iris dataset ?</p>\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: Considering the previous question... are there any splits that are equivalent ? Why/Why not ?</p>\n",
    "\n",
    "<p style=\"background-color:#660066; color:#fff;padding:5px; font-weight:bold\">Q: Go and dig into scikit-learn documentation... What's the other splitting criterion for decision tree classifier (other than the entropy) ?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "You now finished the questions for this laboratory. (nicely done!)\n",
    "Your note will only be based on what is before this!\n",
    "\n",
    "Everything that follows is for fun/learning. We will:\n",
    "- apply decision trees on real data\n",
    "- see how the Random Forest algorithm works\n",
    "\n",
    "Oh and by the way: from now on, no more complicated calculus!\n",
    "Only code execution and general comprehension...\n",
    "\n",
    "So keep calm, keep focused... and read on at your own pace, it will be useful ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic\n",
    "\n",
    "The following problem is about the Titanic. The purpose of this project is to try and predict who survived and who died during the wreckship of the Titanic.\n",
    "\n",
    "To help us with this, we have a training set that is available, with some features such as:\n",
    "- **survival**        Survival (0 = No; 1 = Yes)\n",
    "- **pclass**          Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "- **name**            Name\n",
    "- **sex**             Sex\n",
    "- **age**             Age\n",
    "- **sibsp**           Number of Siblings/Spouses Aboard\n",
    "- **parch**           Number of Parents/Children Aboard\n",
    "- **ticket**          Ticket Number\n",
    "- **fare**            Passenger Fare\n",
    "- **cabin**           Cabin\n",
    "- **embarked**        Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "Let's load the dataset and try to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "\n",
       "                                                Name     Sex  Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male   22      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38      1   \n",
       "2                             Heikkinen, Miss. Laina  female   26      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For .read_csv, always use header=0 when you know row 0 is the header row\n",
    "df = pd.read_csv('train.csv', header=0)\n",
    "\n",
    "# Let's look at the first 3 rows\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 90.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 0 to 890\n",
      "Data columns (total 13 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "AgeFill        891 non-null float64\n",
      "dtypes: float64(3), int64(5), object(5)\n",
      "memory usage: 97.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# We see that the Age column has some missing values\n",
    "# We won't be able to use these values later on...\n",
    "# We can either drop them or fill them (by the median)\n",
    "df['AgeFill'] = df.Age\n",
    "\n",
    "df.loc[df.Age.isnull(), 'AgeFill'] = df.Age.median()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>AgeFill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex  SibSp  Parch     Fare  AgeFill\n",
       "0         0       3    male      1      0   7.2500       22\n",
       "1         1       1  female      1      0  71.2833       38\n",
       "2         1       3  female      0      0   7.9250       26"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's drop some columns that we won't use\n",
    "df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked', 'Age'], axis=1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>AgeFill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex  SibSp  Parch     Fare  AgeFill\n",
       "0         0       3    1      1      0   7.2500       22\n",
       "1         1       1    0      1      0  71.2833       38\n",
       "2         1       3    0      0      0   7.9250       26"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us convert the Sex column from string to int\n",
    "df['Sex'] = df['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived:  342\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('Survived', axis=1).as_matrix()\n",
    "y = df.Survived\n",
    "print 'Survived: ', np.count_nonzero(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To make sure we are not overfitting (https://class.coursera.org/ml-005/lecture/39), we need to separate between a training and a testing set.\n",
    "\n",
    "We can train on the training set, and at the end of the training step, we will look at the performance of the trained model on the testing set to see how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (596, 6) (596,)\n",
      "Test dataset shape: (295, 6) (295,)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.cross_validation as skcv\n",
    "\n",
    "X_train, X_test, y_train, y_test = skcv.train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print 'Train dataset shape:', X_train.shape, y_train.shape\n",
    "print 'Test dataset shape:', X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good thing to do when resolving a problem is to plot information about data (as we did above) and also plot it. It helps to understand what is really going on.\n",
    "\n",
    "Let's look at age variable for example... and let's plot its distribution (histogram)!\n",
    "\n",
    "We see a peak at roughly 30... this is because we filled the missing value by the median!\n",
    "So all the missing values are now equal to the median instead of being missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEDCAYAAAA7jc+ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f561afbdfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = pl.hist(X_train[:,5], bins=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On training set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98       374\n",
      "          1       1.00      0.95      0.97       222\n",
      "\n",
      "avg / total       0.98      0.98      0.98       596\n",
      "\n",
      "On testing set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.79      0.79       175\n",
      "          1       0.69      0.70      0.70       120\n",
      "\n",
      "avg / total       0.75      0.75      0.75       295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "tree_model = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "tree_model = tree_model.fit(X_train, y_train)\n",
    "y_preds_train = tree_model.predict(X_train)\n",
    "y_preds_test = tree_model.predict(X_test)\n",
    "\n",
    "print \"On training set:\"\n",
    "print classification_report(y_train, y_preds_train)\n",
    "print \"On testing set:\"\n",
    "print classification_report(y_test, y_preds_test)\n",
    "\n",
    "# We can have a look at the generated tree by first exporting it to dot file\n",
    "export_graphviz(tree_model, out_file='tree.dot')\n",
    "# Then we can put the generaTted dot file in graphviz\n",
    "# Or use online tools like http://www.webgraphviz.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we are overfitting (0.98 on training set and 0.75 in testing set)... That is bad!\n",
    "\n",
    "Let's see if we can avoid that! We need to tune the hyperparameters of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On training set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.90      0.86       374\n",
      "          1       0.80      0.70      0.75       222\n",
      "\n",
      "avg / total       0.82      0.82      0.82       596\n",
      "\n",
      "On testing set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.90      0.86       175\n",
      "          1       0.83      0.71      0.76       120\n",
      "\n",
      "avg / total       0.82      0.82      0.82       295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tree_model = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "\n",
    "tree_model = tree_model.fit(X_train, y_train)\n",
    "y_preds_train = tree_model.predict(X_train)\n",
    "y_preds_test = tree_model.predict(X_test)\n",
    "\n",
    "print \"On training set:\"\n",
    "print classification_report(y_train, y_preds_train)\n",
    "print \"On testing set:\"\n",
    "print classification_report(y_test, y_preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah! It is better now! We achieved lower performances on the training data, but by doing so, we also have a better understanding of the data we never seen before (test data).\n",
    "\n",
    "This means that this second option is better!\n",
    "\n",
    "In the general case, it is complicated to tune decision trees so that they don't overfit...\n",
    "\n",
    "This is why a great algorithm takes care of that for you! Here come the **Random Forest**!\n",
    "\n",
    "Basically what it does is construct an ensemble of trees (a lot of them). And train each of them on part (generally 66%) of the data. In the end, each single tree overfits a part of the data. But at the end of the training process, each tree casts a vote, and random forest counts the class with maximum votes. The random process therefore \"suppresses\" the overfitting effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "There are a few things to know about random forests:\n",
    "\n",
    "1. The more trees (estimators), the better! More trees is not equivalent to more overfitting!\n",
    "2. You don't compute the training error, you use the OOB (out of bag) error, which is equivalent\n",
    "3. You can parallelize the training since each tree is train individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On training set: 0.807046979866\n",
      "On testing set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.85      0.83       175\n",
      "          1       0.76      0.72      0.74       120\n",
      "\n",
      "avg / total       0.80      0.80      0.80       295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=1000, oob_score=True)\n",
    "rf_model = rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_preds_test = rf_model.predict(X_test)\n",
    "\n",
    "print \"On training set:\", rf_model.oob_score_\n",
    "print \"On testing set:\"\n",
    "print classification_report(y_test, y_preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
